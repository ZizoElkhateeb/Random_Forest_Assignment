# ğŸ“Œ Assignment: Model Optimization and Performance Tuning

1.   List item
2.   List item



# ğŸš€ Solve It Yourself!

This assignment is your chance to think like a data scientist. Donâ€™t rely on AI to do the work for you â€” the real learning happens when you explore, experiment, and problem-solve.

Mistakes are okay â€” theyâ€™re part of the journey. Trust your skills, stay curious, and give it your best shot.

Youâ€™ve got this! ğŸ’ª

## ğŸ¯ Objective:

- Explore Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree (with CCP Post-Pruning), and Random Forest.
- Optimize and compare model performance.

## ğŸ“Œ Hint:

- Make a result dataframe to append to it model name and performance metrics for final comparison (use visualization as well).
---

## ğŸ“ Part 1: Data Preparation
1. **Download a dataset from Kagglehub**.
2. **Load the dataset** and inspect its structure (columns, types, missing values).
3. **Preprocess the data:**
   - Handle missing values
   - Encode categorical variables
   - Scale numeric features

ğŸ‘‰ **Question:** What preprocessing steps did you apply, and why?

---

## ğŸ” Part 2: Model Building

### ğŸ”¹ 2.1 Logistic Regression
- Build a baseline Logistic Regression model.
- **Experiment:** Tune the `C` parameter (regularization strength).

ğŸ‘‰ **Question:** How does changing `C` affect the modelâ€™s performance?

---

### ğŸ”¹ 2.2 K-Nearest Neighbors (KNN)
- Train a KNN model with a default `k=5`.
- **Experiment:**
   - Test different values of `k`.
   - Compare performance using `euclidean` vs. `manhattan` distance.

ğŸ‘‰ **Question:** What is the best `k` for your dataset? Why did it perform better?

---

## ğŸŒ³ Part 3: Decision Tree with Pre-pruning & CCP (Post Pruning)
- Train a Decision Tree with default settings.
- Try pre-pruning hyperparameters.
- Check feature importance attribute.
- Extract `ccp_alpha` values using `cost_complexity_pruning_path`.
- Build pruned trees for different `ccp_alpha` values.

ğŸ‘‰ **Question:** What pre-pruning hyperparameter did you tune? How did you change them to increase performance?

ğŸ‘‰ **Question:** Which `ccp_alpha` value gave the best results, and why?

ğŸ‘‰ **Question:** How did the tree size change after pruning?

---

## ğŸŒ² Part 4: Random Forest
- Train a Random Forest model with 100 trees.
- **Experiment:** Vary `n_estimators` and `max_depth` and other hyperparameters.

ğŸ‘‰ **Question:** How did changing these hyperparameters affect performance?

---

## ğŸ§  Part 5: Model Comparison and Optimization
- Compare all models using Accuracy, Precision, Recall, and F1-score.
- **Reflect:**
   - Which model performed best?
   - How did tuning improve performance?
   - What trade-offs (e.g., overfitting vs. underfitting) did you observe?

ğŸ‘‰ **Question:** Summarize which model you would choose for this dataset and why.

---

## â­ Stretch Goal (Optional):
- Use **GridSearchCV** or **RandomizedSearchCV** to fully optimize one model and retrieve best parameters and best model for each.
- Visualize **feature importance** (especially for Decision Tree/Random Forest).

ğŸ‘‰ **Bonus Question:** Did advanced tuning or feature importance insights change your final model choice?
